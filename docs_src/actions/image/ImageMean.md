# å‡å€?

å‡å€¼æ“ä½œæ˜¯å›¾åƒé¢„å¤„ç†é‡Œéå¸¸å…³é”®çš„æ­¥éª¤ï¼Œä¸»è¦ç›®çš„æ˜¯æ¶ˆé™¤å›¾åƒæ•°æ®ä¸­çš„åå·®ï¼Œè®©æ¨¡å‹èƒ½å¤Ÿæ›´ç¨³å®šåœ°å­¦ä¹ ã€?

### åŸç†

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨çš„å›¾åƒæ•°æ®å¾€å¾€ä¼šç»è¿‡é¢„å¤„ç†ï¼Œå…¶ä¸­å‡å€¼æ“ä½œæ˜¯é‡è¦çš„ä¸€ç¯ã€‚å…·ä½“åšæ³•æ˜¯ï¼Œä»å›¾åƒçš„æ¯ä¸ªåƒç´ å€¼é‡Œå‡å»é¢„å…ˆè®¡ç®—å¥½çš„å‡å€¼ã€‚è¿™æ ·åšå¯ä»¥æŠŠå›¾åƒæ•°æ®çš„ä¸­å¿ƒè°ƒæ•´åˆ°é›¶é™„è¿‘ï¼Œä»è€ŒåŠ å¿«æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦ï¼Œè¿˜èƒ½æå‡æ¨¡å‹çš„ç¨³å®šæ€§ã€?

### ä½¿ç”¨åœºæ™¯

* æ¨¡å‹è®­ç»ƒï¼šåœ¨è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼Œé€šå¸¸ä¼šå¯¹è®­ç»ƒæ•°æ®è®¡ç®—å‡å€¼ï¼Œç„¶ååœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µéƒ½è¿›è¡Œå‡å»å‡å€¼çš„æ“ä½œï¼Œä¿è¯æ•°æ®çš„ä¸€è‡´æ€§ã€?
* æ¨¡å‹æ¨ç†ï¼šåœ¨ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†æ—¶ï¼Œéœ€è¦æŒ‰ç…§æ¨¡å‹è®­ç»ƒæ—¶çš„é¢„å¤„ç†æ–¹å¼ï¼Œå¯¹è¾“å…¥å›¾åƒè¿›è¡Œå‡å»å‡å€¼çš„æ“ä½œï¼Œè¿™æ ·æ‰èƒ½å¾—åˆ°å‡†ç¡®çš„ç»“æœã€?


![ImageMean](./images/17.png ':size=90%')

## å­æµç¨?
> ä¸æ”¯æŒ?


## è¿è¡Œå‚æ•°

* å›¾åƒ
> å¾…å¤„ç†çš„å›¾åƒã€?
* å‡å€?
> æ¯ä¸ªé€šé“çš„å‡å€¼ï¼Œé»˜è®¤ï¼?.485, 0.456, 0.406ï¼‰ï¼Œå¯¹åº” *RGB* æ ¼å¼çš„å›¾åƒã€‚å‰ææ˜¯å›¾åƒåƒç´ å·²ç¼©æ”¾è‡³ *0~1*ã€‚å¦‚æœå›¾åƒåƒç´ æ²¡æœ‰ç¼©æ”¾ï¼ˆå›¾åƒé»˜è®¤åƒç´ èŒƒå›´æ˜?*0~255*ï¼‰ï¼Œåˆ™ä½¿ç”¨ç±»ä¼¼ï¼š`(0.485, 0.456, 0.406ï¼? 255 =ï¼?23.680ï¼?16.779, 103.939)` å€¼ã€?



## è¾“å‡º

> å‡å€¼åçš„çš„å›¾åƒï¼Œå‚è€?[`Image`](/types/Image.md)ã€?


## è„šæœ¬è°ƒç”¨

```python
import simple;

```

## èµ„æº

!> å¦‚æœå°†é»˜è®¤å‡å€¼åº”ç”¨åˆ°æ‚¨è‡ªå·±çš„æ•°æ®é›†ä¸­ï¼Œå¯èƒ½ä¸ä¼šè·å¾—è‰¯å¥½çš„æ€§èƒ½ï¼Œå› ä¸ºè¿™äº›ç»Ÿè®¡æ•°æ®å±äº?*ImageNet*ï¼?æ‚¨éœ€è¦æ ¹æ®ä¸åŒçš„é¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä½¿ç”¨ä¸åŒçš„å‡å€¼ã€?


### [how to calculate the mean and std in the DataLoader? Here I give two ways](https://xydida.com/2022/9/11/ComputerVision/Normalize-images-with-transform-in-pytorch-dataloader/):

1. Calculate mean and std of the three channels in each batch and average them at the end.

```

def mean_std_for_loader1(loader: DataLoader):
    mean = torch.zeros(3)
    std = torch.zeros(3)
    for X, _ in loader:
        for d in range(3):
            mean[d] += X[:, d, :, :].mean()
            std[d] += X[:, d, :, :].std()
    mean.div_(len(loader))
    std.div_(len(loader))
    return list(mean.numpy()), list(std.numpy())

means, stds = mean_std_for_loader1(train_dataloader)
print(means)
print(stds)

# Output
# [0.47921667, 0.44638008, 0.40927842]
# [0.26486507, 0.25691825, 0.2580299]
```

2. The std can be derivated from the mean of square of the data and square of the mean of the data, this is referenced from Jorrit Willaert [1]. Here is the formula:

```
def mean_std_for_loader(loader: DataLoader):
    # var[X] = E[X**2] - E[X]**2
    channels_sum, channels_sqrd_sum, num_batches = 0, 0, 0
    for data, _ in tqdm(loader):
        this_batch_size = data.size()[0]
        weight = this_batch_size / loader.batch_size
        channels_sum += weight*torch.mean(data, dim=[0, 2, 3])
        channels_sqrd_sum += weight*torch.mean(data ** 2, dim=[0, 2, 3])
        num_batches += weight

    mean = channels_sum / num_batches
    std = (channels_sqrd_sum / num_batches - mean ** 2) ** 0.5
    return mean, std

means, stds = mean_std_for_loader(train_dataloader)
print(means)
print(stds)

# Output
# tensor([0.4786, 0.4459, 0.4088])
# tensor([0.2656, 0.2577, 0.2589])
```
